{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of beginner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "# Midterm Review\n",
        "##### By: Pieter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-RZgtW6yRSM"
      },
      "source": [
        "To do:\n",
        "ROC\n",
        "AOC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Nq2cHuumxC"
      },
      "source": [
        "### Assignment Quick Look up\n",
        "\n",
        "- 01: Supervised learning, Linear models, and Loss functions\n",
        "- 02: Maximum Likelihood\n",
        "- 03: Classification with Logistic Regression\n",
        "- 04: Confidence Intervals & The Bootstrap\n",
        "- 05: Model Selection & Cross Validation\n",
        "- 06: Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxbS9baeqGL3"
      },
      "source": [
        "### Loss Function\n",
        "- OLS: Ordinary Least Square\n",
        "- LAD: Least Absolute Square\n",
        "\n",
        "##### Loss Function Table\n",
        "\n",
        "| OLS | LAD |\n",
        "|--|-|\n",
        "|Not very Robust | Robust |\n",
        "|Stable Solution| Unstable Solution|\n",
        "| One Solution | Possibly Multiple Solutions|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dAoqSruwuFo"
      },
      "source": [
        "### Likelihood\n",
        "- Measure the goodness of fit of a statistical model \n",
        "- Given data how well does the distribution fit the data?\n",
        "- Data is fixed\n",
        "\n",
        "#### Probabilty\n",
        "- The chances of an event of occuring \n",
        "- Given a distribution what is the chance of the event occuring\n",
        "- Distribution is fixed\n",
        "\n",
        "\n",
        "### Maximum Likelihood\n",
        "Maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQo3nO3RTYvf"
      },
      "source": [
        "### Bootstraping\n",
        "\n",
        "Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. \n",
        "\n",
        "This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.\n",
        "\n",
        "```python\n",
        "\n",
        "# Write a Bootstrap function that records the fitted models \n",
        "def BootstrapCoef(data,numboot=1000):\n",
        "    regr = sklearn.linear_model.LinearRegression()\n",
        "    #numboot = 1000\n",
        "    n = len(data)\n",
        "    theta = np.zeros((numboot,2))    \n",
        "    for i in range(numboot):\n",
        "        d = data.sample(n, replace=True)\n",
        "        X_fit = np.c_[d.ht]\n",
        "        regr.fit(X_fit,d.wt)\n",
        "        theta[i,0]=regr.intercept_\n",
        "        theta[i,1]=regr.coef_\n",
        "    return theta\n",
        "\n",
        "params = BootstrapCoef(df,100)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjeL-QT_Pi16"
      },
      "source": [
        "### Cross Validation \n",
        "Croos validation splits the data into testing and training set\n",
        "\n",
        "Training Data:\n",
        "Is used to estimate the paramters for the machine learning methods\n",
        "\n",
        "Testing Data:\n",
        "Used to evaluate how well the machine learning method works\n",
        "\n",
        "Cross Validation Size:\n",
        "\n",
        "| Paramteter| 2 Folds | 5-10 Folds | N Folds |\n",
        "|-|-|-|-|\n",
        "|Overestimation bias of prediction error| Bad | Present | Nearly Unbiased |\n",
        "|Computational Cost | Low | Mid | High | \n",
        "|Variance of estimate| Low | Low | High |\n",
        "\n",
        "```python\n",
        "# Split Data into Training and Testing Data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = test_size, random_state = 0)\n",
        "\n",
        "# Cross Validation Error\n",
        "sc = sk.metrics.make_scorer(mean_squared_error)\n",
        "linear_mean_cv_scores = cross_val_score(linear_pipeline, poly_train_set[i], y_train, cv=10, scoring=sc)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nt4vyr0v8Y3"
      },
      "source": [
        "### Regresions\n",
        "\n",
        "### Linear Regression:\n",
        "Creates a line of best fit\n",
        "\n",
        "```python\n",
        "linear_poly_pipeline = Pipeline([('poly_features', PolynomialFeatures(degree=2)),\n",
        "                                  ('LR', LinearRegression())])\n",
        "linear_pipeline.fit(x_train, y_train)\n",
        "\n",
        "```\n",
        "\n",
        "### Ridge Regression (L2):\n",
        "\n",
        "Ridge regression adds a pentaly function on the slope\n",
        "\n",
        "Ridge regression = Sum of Square Residuals + $\\lambda \\times slope^2$\n",
        "\n",
        "Ridge regression introduces bias in the model to potentially reduce variance of the testing data\n",
        "\n",
        "Ridge Regression can shrink slope asymptotically close to 0\n",
        "Ridge Regression is better when most features are useful\n",
        "\n",
        "```python\n",
        "ridge_pipeline = Pipeline([('poly_features', PolynomialFeatures()),\n",
        "                           ('scaler', StandardScaler()), \n",
        "                           ('ridge_regression', Ridge(alpha=np.exp(2)))])\n",
        "\n",
        "ridge_pipeline.fit(x_train, y_train)\n",
        "yp_ridge = ridge_pipeline.predict(x_test)\n",
        "```\n",
        "\n",
        "### Lasso Regression (L1):\n",
        "\n",
        "Lasso regression adds a pentaly function on the slope\n",
        "\n",
        "Lasso regression = Sum of Square Residuals + $\\lambda \\times |slope|$\n",
        "\n",
        "Lasso regression can shrink slope to 0\n",
        "Lasso regression is better when model features are useless. This makes it better than ridge regression at reducing variance. \n",
        "\n",
        "Simplifies \n",
        "```python\n",
        "lasso_pipeline = Pipeline([('poly_features', PolynomialFeatures()), \n",
        "                           ('scaler', StandardScaler()), \n",
        "                           ('lasso', sk.linear_model.Lasso(alpha=np.exp(2)))])\n",
        "\n",
        "lasso_pipeline.fit(x_train, y_train)\n",
        "```\n",
        "\n",
        "### Logistic Regression:\n",
        "\n",
        "Logistic Regression is used to make a true / false outcome based based on input parameters \n",
        "\n",
        "An outcome is marked positive / true if the probability of likehood is greater than 0.50. The threshold of 0.50 can be raised or lowered if required\n",
        "```python\n",
        "LogisticRegression(solver='lbfgs',penalty = 'none',max_iter=10000)\n",
        "\n",
        "# True / False Predict\n",
        "yp_ =lr_amount.predict(xp)\n",
        "\n",
        "# Probabilty Predict\n",
        "yp=lr_amount.predict_proba(xp)\n",
        "\n",
        "# Plotting \n",
        "sns.lineplot(xp,yp)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRUGwJKuoVM7"
      },
      "source": [
        "### Functions \n",
        "\n",
        "#### $R^2$\n",
        "```python\n",
        "def R_Squared(y_true, y_pred):\n",
        "    rss = sum((y_true - y_pred)**2) \n",
        "    tss = sum(( y_true - y_true.mean())**2)\n",
        "    r2 = 1 - rss/tss\n",
        "    return r2\n",
        "```\n",
        "\n",
        "#### $RSS$\n",
        "Residual Sum of Squares\n",
        "```python\n",
        "def RSS(y_true, y_pred):\n",
        "    residual_sum_of_squares = sum((y_true - y_pred)**2) \n",
        "    return residual_sum_of_squares\n",
        "\n",
        "```\n",
        "\n",
        "#### $TSS$\n",
        "Total Sum of Squares\n",
        "```python\n",
        "def TSS(y_true):\n",
        "    total_sum_of_squares = sum(( y_true - y_true.mean())**2) \n",
        "    return total_sum_of_squares\n",
        "\n",
        "```\n",
        "\n",
        "#### $Confidence Interval$\n",
        "Confidence Interval is the range of values we are fairly confident the true value lies in\n",
        "\n",
        "Extra:\n",
        "When confidence intervals overlap a t test must be preformed to determine if the estamands are statistically significant difference \n",
        "\n",
        "ie: An interval that covers 95% of the means \n",
        "\n",
        "```python\n",
        "#Compute the errors and a point estimate of the generalization error\n",
        "test_errors = np.square(y_test - yp_ridge)\n",
        "generalization_error = test_errors.mean()\n",
        "\n",
        "#Construct a confidence interval\n",
        "test_ci = generalization_error + 1.96 * np.std(test_errors) / np.sqrt(len(test_errors)) * np.array([-1, 1])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmkMNvuyqWet"
      },
      "source": [
        "### Plots \n",
        "\n",
        "#### Scatter Plot\n",
        "```python\n",
        "def scatter_plot(x_data_pts, y_data_pts):\n",
        "  # Plot\n",
        "  fig, ax = plt.subplots(dpi = 120)\n",
        "  fig.set_facecolor('white')\n",
        "\n",
        "  # Plot Formatting \n",
        "  ax.set_title('X Title vs. Y Title')\n",
        "  ax.set_xlabel('X Title')\n",
        "  ax.set_ylabel('Y Title')\n",
        "\n",
        "  # Plot Data\n",
        "  ax.plot(x_data_pts,y_data_pts, 'k.', label='Data Set Name')\n",
        "\n",
        "  # Legend\n",
        "  ax.legend(loc=1)\n",
        "  plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "#### Histograms:\n",
        "\n",
        "```python\n",
        "hist = sns.distplot(slope)\n",
        "\n",
        "hist.set_title(\"Slope Distribution\")\n",
        "hist.set_xlabel(\"Slope Value\")\n",
        "hist.set_ylabel(\"Probability\")\n",
        "hist.set_facecolor('white')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Q4nDoPsJFa"
      },
      "source": [
        "### Data Cleaning\n",
        "Iterate across all data inside a data frame\n",
        "```python\n",
        "# Check for NaN\n",
        "for (columnName, columnData) in model_data.iteritems(): \n",
        "    model_data = model_data[~np.isnan(model_data[columnName])]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orv8aKuKYjYF"
      },
      "source": [
        "### Data Frame Building\n",
        "Build data frame from np arrays\n",
        "\n",
        "```python\n",
        "# Build Data Frame to extract Coef Columns by Name\n",
        "p = PolynomialFeatures(degree=2).fit(x_test)\n",
        "features = pd.DataFrame(ridge_coefs, columns=p.get_feature_names(x_data.columns))\n",
        "```"
      ]
    }
  ]
}